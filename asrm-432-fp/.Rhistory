test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#first hundred observations.
train_selected = train_data[1:100, ]
##Bagging
#apply bagging method (replace train_selected with train_data)
bagging = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntrees
plot(bagging, col = "darkorange")
#ntree = 340 looks decent
abline(v = 340)
#bagging with the best ntree
bagging2 = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging2, newdata = test_data)
#confusion matrix and accuracy (training data)
conf_tab = bagging2$confusion[, 1:2]
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
sum(diag(conf_tab2)) / sum(conf_tab2)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
# X33, X6, X41, X29, X24
##Random Forest (mtry < 85)
#tune mtry with ntree with ntree = 340
tuned = tuneRF(x = train_selected[, -86], y = as.factor(train_selected$X86),
ntreeTry = 340, mtryStart = 42, stepFactor = 1.5, trace = FALSE)
tuned_mtry = as_tibble(tuned) |>
arrange(OOBError) |>
select(mtry) |>
head(1) |>
pull(mtry)
#choose ntrees = 340 with tuned mtry
rf_tuned = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = tuned_mtry, importance = TRUE)
#random forest predictions (w/ tuned)
rf_tuned_pred = predict(bagging_tuned, newdata = test_data)
library(tidyverse)
library(RCurl)
library(rpart)
library(randomForest)
library(caret)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#first hundred observations.
train_selected = train_data[1:100, ]
##Bagging
#apply bagging method (replace train_selected with train_data)
bagging = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntrees
plot(bagging, col = "darkorange")
#ntree = 340 looks decent
abline(v = 340)
#bagging with the best ntree
bagging2 = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging2, newdata = test_data)
#confusion matrix and accuracy (training data)
conf_tab = bagging2$confusion[, 1:2]
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
sum(diag(conf_tab2)) / sum(conf_tab2)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
# X33, X6, X41, X29, X24
##Random Forest (mtry < 85)
#tune mtry with ntree with ntree = 340
tuned = tuneRF(x = train_selected[, -86], y = as.factor(train_selected$X86),
ntreeTry = 340, mtryStart = 42, stepFactor = 1.5, trace = FALSE)
tuned_mtry = as_tibble(tuned) |>
arrange(OOBError) |>
select(mtry) |>
head(1) |>
pull(mtry)
#choose ntrees = 340 with tuned mtry
rf_tuned = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = tuned_mtry, importance = TRUE)
#random forest predictions (w/ tuned)
rf_tuned_pred = predict(rf_tuned, newdata = test_data)
#confusion matrix and accuracy (over the training data) (go over with prof or ta)
conf_tab_tuned = rf_tuned$confusion[, 1:2]
sum(diag(conf_tab_tuned)) / sum(conf_tab_tuned)
library(tidyverse)
library(RCurl)
library(rpart)
library(randomForest)
library(caret)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#first hundred observations.
train_selected = train_data[1:100, ]
##Bagging
#apply bagging method (replace train_selected with train_data)
bagging = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntrees
plot(bagging, col = "darkorange")
#ntree = 340 looks decent
abline(v = 340)
#bagging with the best ntree
bagging2 = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging2, newdata = test_data)
#confusion matrix and accuracy (training data)
conf_tab = bagging2$confusion[, 1:2]
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
sum(diag(conf_tab2)) / sum(conf_tab2)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
# X33, X6, X41, X29, X24
##Random Forest (mtry < 85)
#tune mtry with ntree with ntree = 340
tuned = tuneRF(x = train_selected[, -86], y = as.factor(train_selected$X86),
ntreeTry = 340, mtryStart = 42, stepFactor = 1.5, trace = FALSE)
tuned_mtry = as_tibble(tuned) |>
arrange(OOBError) |>
select(mtry) |>
head(1) |>
pull(mtry)
#choose ntrees = 340 with tuned mtry
rf_tuned = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = tuned_mtry, importance = TRUE)
#random forest predictions (w/ tuned)
rf_tuned_pred = predict(rf_tuned, newdata = test_data)
#confusion matrix and accuracy (over the training data) (go over with prof or ta)
conf_tab_tuned = rf_tuned$confusion[, 1:2]
sum(diag(conf_tab_tuned)) / sum(conf_tab_tuned)
#or
conf_tab_tuned = table(Predicted = bagging_tuned_pred, Actual = train_data$X86[1:4000])
library(tidyverse)
library(RCurl)
library(rpart)
library(randomForest)
library(caret)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#first hundred observations.
train_selected = train_data[1:100, ]
##Bagging
#apply bagging method (replace train_selected with train_data)
bagging = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntrees
plot(bagging, col = "darkorange")
#ntree = 340 looks decent
abline(v = 340)
#bagging with the best ntree
bagging2 = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging2, newdata = test_data)
#confusion matrix and accuracy (training data)
conf_tab = bagging2$confusion[, 1:2]
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
sum(diag(conf_tab2)) / sum(conf_tab2)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
# X33, X6, X41, X29, X24
##Random Forest (mtry < 85)
#tune mtry with ntree with ntree = 340
tuned = tuneRF(x = train_selected[, -86], y = as.factor(train_selected$X86),
ntreeTry = 340, mtryStart = 42, stepFactor = 1.5, trace = FALSE)
tuned_mtry = as_tibble(tuned) |>
arrange(OOBError) |>
select(mtry) |>
head(1) |>
pull(mtry)
#choose ntrees = 340 with tuned mtry
rf_tuned = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = tuned_mtry, importance = TRUE)
#random forest predictions (w/ tuned)
rf_tuned_pred = predict(rf_tuned, newdata = test_data)
#confusion matrix and accuracy (over the training data) (go over with prof or ta)
conf_tab_tuned = rf_tuned$confusion[, 1:2]
sum(diag(conf_tab_tuned)) / sum(conf_tab_tuned)
#or
conf_tab_tuned = table(Predicted = rf_tuned_pred, Actual = train_data$X86[1:4000])
conf_tab_tuned
sum(diag(conf_tab_tuned)) / sum(conf_tab_tuned)
#confusion matrix and accuracy (over the target data)
conf_tab_tuned2 = table(Predicted = rf_tuned_pred, Actual = target_data$X1)
conf_tab_tuned2
sum(diag(conf_tab_tuned2)) / sum(conf_tab_tuned2)
nrow(train_dtata)
nrow(train_data)
#split the training data
train = train_data[1:4000, ]
test = train_data[-1:4000,]
test = train_data[4001:nrow(test_data),]
nrow(train_data)
#split the training data
train = train_data[1:4000, ]
test = train_data[4001:5822,]
#apply bagging method
bagging = randomForest(as.factor(train_selected$X86) ~., data = train,
mtry = 85, importance = TRUE)
#apply bagging method
bagging = randomForest(as.factor(train_selected$X86) ~., data = train,
mtry = 85, importance = TRUE)
#apply bagging method
bagging = randomForest(as.factor(train$X86) ~., data = train,
mtry = 85, importance = TRUE)
#confusion matrix and accuracy (over the training data)
conf_tab = table(Predicted = bagging_pred, Actual = test$X86)
#confusion matrix and accuracy (over the training data)
conf_tab = table(Predicted = bagging_pred, Actual = test$X86)
test
#bagging with the best ntree
bagging2 = randomForest(as.factor(train$X86) ~., data = train,
ntree = 340, mtry = 85, importance = TRUE)
#split the training data
train = train_data[1:4000, ]
test = train_data[4001:5822,]
#apply bagging method
bagging = randomForest(as.factor(train$X86) ~., data = train,
mtry = 85, importance = TRUE)
#set seed to be 7
set.seed(7)
#split the training data
train = train_data[1:4000, ]
test = train_data[4001:5822,]
#apply bagging method
bagging = randomForest(as.factor(train$X86) ~., data = train,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntrees
plot(bagging, col = "darkorange")
#ntree = 340 looks decent
abline(v = 340)
#ntree = 240 looks decent
abline(v = 240)
#bagging with the best ntree
bagging = randomForest(as.factor(train$X86) ~., data = train,
ntree = 240, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging, newdata = test)
#confusion matrix and accuracy (over the training data)
conf_tab = table(Predicted = bagging_pred, Actual = test$X86)
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (over the training data)
conf_tab = table(Predicted = bagging_pred, Actual = test$X86)
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
#confusion matrix and accuracy (over the training data)
conf_tab = table(Predicted = bagging_pred, Actual = test$X86)
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
sum(diag(conf_tab2)) / sum(conf_tab2)
#confusion matrix and accuracy (over the training data)
conf_tab = table(Predicted = bagging_pred, Actual = test$X86)
sum(diag(conf_tab)) / sum(conf_tab)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
#tune mtry with ntree with ntree = 240
tuned = tuneRF(x = train[, -86], y = as.factor(train$X86),
ntreeTry = 240, mtryStart = 42, stepFactor = 1.5, trace = FALSE)
tuned_mtry = as_tibble(tuned) |>
arrange(OOBError) |>
select(mtry) |>
head(1) |>
pull(mtry)
#choose ntrees = 340 with tuned mtry
rf_tuned = randomForest(as.factor(train$X86) ~., data = train,
ntree = 340, mtry = tuned_mtry, importance = TRUE)
#random forest predictions (w/ tuned)
rf_tuned_pred = predict(rf_tuned, newdata = test)
tuned_mtry
tuned
#mtry = 28
rf_28 = randomForest(as.factor(train$X86) ~., data = train,
mtry = 28, importance = TRUE)
#mtry's = 28, 42, 63
mtry_values = c(28, 42, 63)
#initialize accuracy vector
accuracy_results = numeric(length(mtry_values))
for (i in seq_along(mtry_values)) {
#train rf model with mtry
rf_model = randomForest(as.factor(X86) ~ ., data = train, mtry = mtry_values[i])
#predict on the test
predictions = predict(rf_model, newdata = test)
#calculate accuracy
accuracy = mean(predictions == test$X86)
#store accuracy
accuracy_results[i] = accuracy
}
accuracy_results
names(accuracy_results)
for (i in seq_along(mtry_values)) {
#train rf model with mtry
rf_model = randomForest(as.factor(X86) ~ ., data = train, mtry = mtry_values[i])
#predict on the test
predictions = predict(rf_model, newdata = test)
#calculate accuracy
accuracy = mean(predictions == test$X86)
#store accuracy
accuracy_results[i] = accuracy
}
#split the training data
train_ind <- sample(seq_len(nrow(mtcars)), size = smp_size)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#split the training data
train_ind = sample(seq_len(nrow(train_data)), size = 4000)
train = train_data[train_ind]
#split the training data
train_ind = sample(seq_len(nrow(train_data)), size = 4000)
train = train_data[train_ind]
#split the training data
train_ind = sample(seq_len(nrow(train_data)), size = 4000)
train = train_data[train_ind,]
test = train_data[-train_ind,]
#apply bagging method
bagging = randomForest(as.factor(train$X86) ~., data = train,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntree
plot(bagging, col = "darkorange")
#ntree = 240 looks decent
abline(v = 240)
#bagging with the best ntree
bagging = randomForest(as.factor(train$X86) ~., data = train,
ntree = 240, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging, newdata = test)
library(tidyverse)
library(RCurl)
library(rpart)
library(randomForest)
library(caret)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#split the training data
train_ind = sample(seq_len(nrow(train_data)), size = 4000)
train = train_data[train_ind,]
test = train_data[-train_ind,]
##Bagging
#apply bagging method
bagging = randomForest(as.factor(train$X86) ~., data = train,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntree
plot(bagging, col = "darkorange")
#ntree = 240 looks decent
abline(v = 240)
#bagging with the best ntree
bagging = randomForest(as.factor(train$X86) ~., data = train,
ntree = 240, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging, newdata = test)
#confusion matrix and accuracy (over the training data)
conf_tab_bg = table(Predicted = bagging_pred, Actual = test$X86)
sum(diag(conf_tab_bg)) / sum(conf_tab_bg)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
# X33, X6, X41, X29, X24
#confusion matrix and accuracy (over the target data)
bagging_pred_target = predict(bagging, newdata = test_data)
conf_tab_bg_actual = table(Predicted = bagging_pred_target, Actual = test_data$X86)
library(tidyverse)
library(RCurl)
library(rpart)
library(randomForest)
library(caret)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#split the training data
train_ind = sample(seq_len(nrow(train_data)), size = 4000)
train = train_data[train_ind,]
test = train_data[-train_ind,]
##Bagging
#apply bagging method
bagging = randomForest(as.factor(train$X86) ~., data = train,
mtry = 85, importance = TRUE)
#plot out-of-bag error to find the best ntree
plot(bagging, col = "darkorange")
#ntree = 240 looks decent
abline(v = 240)
#bagging with the best ntree
bagging = randomForest(as.factor(train$X86) ~., data = train,
ntree = 240, mtry = 85, importance = TRUE)
bagging_pred = predict(bagging, newdata = test)
#confusion matrix and accuracy (over the training data)
conf_tab_bg = table(Predicted = bagging_pred, Actual = test$X86)
sum(diag(conf_tab_bg)) / sum(conf_tab_bg)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
# X33, X6, X41, X29, X24
#confusion matrix and accuracy (over the target data)
bagging_pred_target = predict(bagging, newdata = test_data)
conf_tab_bg_actual = table(Predicted = bagging_pred_target, Actual = test_data$X86)
#confusion matrix and accuracy (over the target data)
bagging_pred_target = predict(bagging, newdata = test_data)
conf_tab_bg_actual = table(Predicted = bagging_pred_target, Actual = target_data$X1)
sum(diag(conf_tab_bg_actual)) / sum(conf_tab_bg_actual)
#confusion matrix and accuracy (over the target data)
bagging_pred_target = predict(bagging, newdata = test_data)
#confusion matrix and accuracy (over the target data)
bagging_pred_target = predict(bagging, newdata = test_data)
conf_tab_bg_actual = table(Predicted = bagging_pred_target, Actual = target_data$X1)
sum(diag(conf_tab_bg_actual)) / sum(conf_tab_bg_actual)
#tune mtry with ntree with ntree = 240
tuned = tuneRF(x = train[, -86], y = as.factor(train$X86),
ntreeTry = 240, mtryStart = 42, stepFactor = 1.5, trace = FALSE)
#mtry's = 28, 42, 63
mtry_values = c(28, 42, 63)
#initialize accuracy vector
accuracy_results = numeric(length(mtry_values))
for (i in seq_along(mtry_values)) {
#train rf model with mtry
rf_model = randomForest(as.factor(X86) ~ ., data = train, mtry = mtry_values[i])
#predict on the test
rf_pred = predict(rf_model, newdata = test)
#calculate accuracy
accuracy = mean(rf_pred == test$X86)
#store accuracy
accuracy_results[i] = accuracy
}
rf_pred_target = predict(rf_model, newdata = test_data)
#confusion matrix and accuracy (over the target data)
conf_tab_rf_target = table(Predicted = rf_pred, Actual = target_data$X1)
rf_pred_target = predict(rf_model, newdata = test_data)
#confusion matrix and accuracy (over the target data)
conf_tab_rf_target = table(Predicted = rf_pred_target, Actual = target_data$X1)
sum(diag(conf_tab_rf_target)) / sum(conf_tab_rf_target)
accuracy_results
rf_pred_target = predict(rf_model, newdata = test_data)
#confusion matrix and accuracy (over the target data)
conf_tab_rf_target = table(Predicted = rf_pred_target, Actual = target_data$X1)
sum(diag(conf_tab_rf_target)) / sum(conf_tab_rf_target)
